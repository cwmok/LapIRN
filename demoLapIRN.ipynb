{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demoLapIRN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idhamari/LapIRN/blob/master/demoLapIRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmRrG-6Dkq4"
      },
      "source": [
        "# 3D Medical Image Registration with DNN\n",
        "\n",
        "Using Laplacian Pyramid Deep Learning Networks (LAPIRN)\n",
        "\n",
        "This is a demo notebook for a github repository [LabIRN](https://github.com/cwmok/LapIRN/blob/master/Code/Train_LapIRN_disp.py). The original code author is [Tony Chi Wing MOK](https://cwmok.github.io/). If you have questions please open issue in the original repository. \n",
        "\n",
        "the paper can be downloaded from [here](https://arxiv.org/abs/2006.16148).\n",
        "\n",
        "\n",
        "The dataset used is from [learn2reg challenge](https://learn2reg.grand-challenge.org/Datasets). It can be downloaded from ()\n",
        "\n",
        "Notes:\n",
        "  - some bugs are fixed.\n",
        "\n",
        "Todos:\n",
        "  - use tensorboard to draw training and testing curves \n",
        "  - improve the reading/writing procedures\n",
        "  - remove redundant code\n",
        "\n",
        "\n",
        "**This notebook is controbuted by:** Ibraheem Al-Dhamari\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIvFGWlcGwla"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W_x8y2tkiU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ffe81-480f-4993-da06-5afd1015ce44"
      },
      "source": [
        "# you can also work with your google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "firstRun = 1 \n",
        "\n",
        "if firstRun:\n",
        "    !pip3 install -U scikit-learn\n",
        "    !pip3 install simpleitk\n",
        "    !pip3 install tensorflow==1.14 tensorflow-gpu==1.14 keras==2.3.1\n",
        "    !pip3 install nibabel tqdm\n",
        "    !pip3 install torchvision torch==1.3.0 \n",
        "    ##clone the updated code\n",
        "    !mkdir LapIRN_org\n",
        "    !git clone https://github.com/idhamari/LapIRN.git LapIRN_org/LapIRN\n",
        "    print('-------------------')\n",
        "    !ls LapIRN_org/LapIRN\n",
        "    print(\"cloning repository is done!\")\n",
        "    \n",
        "    # download the dataset    \n",
        "    !mkdir LapIRN_org/datasets\n",
        "    # note use the same lines with changing the only the id\n",
        "    !curl -c /tmp/cookies \"https://drive.google.com/uc?export=download&id=17uysjRAiXMIT2QApW5kHWP1aHCi5_lPO\" > tmp.txt\n",
        "    !curl -L -b /tmp/cookies \"https://drive.google.com$(cat tmp.txt | grep -Po 'uc-download-link\" [^>]* href=\"\\K[^\"]*' | sed 's/\\&amp;/\\&/g')\" >  LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144.zip\n",
        "    !ls  LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144.zip -l --block-size=M \n",
        "    !unzip LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144.zip -d LapIRN_org/datasets \n",
        "    !ls   LapIRN_org/datasets\n",
        "    print(\"downloading dataset is done!\")\n",
        "    firstRun = 0\n",
        "\n",
        "\n",
        "import time , shutil, os\n",
        "from google.colab import files\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "print(\"      important note: copy or download the results if you want to save them\")\n",
        "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
            "Collecting simpleitk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/6b/85df5eb3a8059b23a53a9f224476e75473f9bcc0a8583ed1a9c34619f372/SimpleITK-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 63kB/s \n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.0.2\n",
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3MB 93kB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/67/559ca8408431c37ad3a17e859c8c291ea82f092354074baef482b98ffb7b/tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1MB 19kB/s \n",
            "\u001b[?25hCollecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow, tensorflow-gpu, keras\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/0b/9d33aef363b6728ad937643d98be713c6c25d50ce338678ad57cee6e6fd5/torch-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 15kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.3.0\n",
            "Cloning into 'LapIRN_org/LapIRN'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 141 (delta 71), reused 113 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 49.46 MiB | 30.02 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "-------------------\n",
            "Code  Data  demoLapIRN.ipynb  demoLapIRN_org.py  LICENSE.md  Model  README.md\n",
            "cloning repository is done!\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3299    0  3299    0     0  10406      0 --:--:-- --:--:-- --:--:-- 10406\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   3162      0 --:--:-- --:--:-- --:--:--  3162\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  371M    0  371M    0     0  17.0M      0 --:--:--  0:00:21 --:--:-- 40.1M\n",
            "-rw-r--r-- 1 root root 372M Apr 14 18:50 LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144.zip\n",
            "Archive:  LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144.zip\n",
            "   creating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/\n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0001.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0001_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0002.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0002_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0003.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0003_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0004.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0004_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0005.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0005_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0006.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0006_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0007.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0007_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0008.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0008_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0009.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0009_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0010.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0010_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0021.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0021_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0022.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0022_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0023.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0023_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0024.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0024_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0025.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0025_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0026.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0026_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0027.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0027_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0028.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0028_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0029.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0029_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0030.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0030_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0031.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0031_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0032.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0032_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0033.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0033_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0034.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0034_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0035.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0035_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0036.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0036_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0037.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0037_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0038.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0038_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0039.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0039_seg.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0040.nii.gz  \n",
            "  inflating: LapIRN_org/datasets/L2R_Task3_AbdominalCT_160x192x144/img0040_seg.nii.gz  \n",
            "L2R_Task3_AbdominalCT_160x192x144  L2R_Task3_AbdominalCT_160x192x144.zip\n",
            "downloading dataset is done!\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "      important note: copy or download the results if you want to save them\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQtPocugG0eE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZnWL1uk6xfW",
        "outputId": "7d77ffbd-9e49-44d9-84ae-82ae3e649f07"
      },
      "source": [
        "import time , shutil, os\n",
        "from google.colab import files\n",
        "%matplotlib inline\n",
        "\n",
        "# testing download a folder and a file \n",
        "\n",
        "# !apt install nano\n",
        "# !nano myTxt14042021.txt\n",
        "#files.upload()  # or use the left side panel \n",
        "!ls\n",
        "# work_directory\n",
        "wd_path      ='/content/LapIRN_org/'\n",
        "# # dataset\n",
        "dataset_path = '../../datasets/L2R_Task3_AbdominalCT_160x192x144'\n",
        "# # 30 images,  image size =  192,160,256, each has its segmentation, the segmentation has 13 segmentation classes,\n",
        "# images are already resized to fit the GPU memory \n",
        "\n",
        "scriptPath = wd_path + 'LapIRN/demoLapIRN_org.py'\n",
        "doTrain     = \" 1 \"\n",
        "slvl1       = \" 0 \"  # start epoch lvl1    \n",
        "slvl2       = \" 0 \"  # start epoch lvl2       \n",
        "slvl3       = \" 0 \"  # start epoch lvl3     \n",
        "lvl1        = \" 3000 \"   # number of iterations for first resolution  \n",
        "lvl2        = \" 3000 \"  # number of iterations for second resolution  \n",
        "lvl3        = \" 30000 \" # number of iterations for third resolution \n",
        "checkpoint  = \" 500 \"# model will be saved after each checkpoint steps\n",
        "\n",
        "\n",
        "#!python $scriptPath $doTrain $slvl1 $slvl2 $slvl3 $lvl1 $lvl2 $lvl3 $checkpoint $wd_path $dataset_path\n",
        "cmd = \"python \" + scriptPath + \" \" + doTrain +slvl1 +slvl2 +slvl3 +lvl1 +lvl2 +lvl3 +checkpoint  + \" \" +wd_path  + \" \" +dataset_path\n",
        "os.system(cmd)\n",
        "\n",
        "#save data every one hour\n",
        "#shutil.rmtree('/content/LapIRN_org/LapIRN/Model/StageNAN')\n",
        "\n",
        "print(\"process started ....\")\n",
        "while 1:\n",
        "      if os.path.isfile('model_folder_compressed.zip'):\n",
        "         os.remove('model_folder_compressed.zip')\n",
        "      print(\"creating archive ...............\")         \n",
        "      time.sleep(60*60)\n",
        "      shutil.make_archive('model_folder_compressed', 'zip', '/content/LapIRN_org/LapIRN/Model')      \n",
        "      print(\"downloading archive ...............\")         \n",
        "      time.sleep(15*60)\n",
        "      files.download('model_folder_compressed.zip') \n",
        "      time.sleep(15*300)\n",
        "\n",
        "#     \n",
        "# # download the pretrained model\n",
        "print(\"done! ....................\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LapIRN_org  model_folder_compressed.zip  sample_data  tmp.txt\n",
            "process started ....\n",
            "creating archive ...............\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdfX-MlzG5Cf"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdyfGPNuCeU"
      },
      "source": [
        "#run testing on testing dataset and save results "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OjoHH2uG8Gf"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWrc1W3FG-h7"
      },
      "source": [
        "#Draw results "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}