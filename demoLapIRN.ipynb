{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demoLapIRN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idhamari/LapIRN/blob/master/demoLapIRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmRrG-6Dkq4"
      },
      "source": [
        "# 3D Medical Image Registration with DNN\n",
        "\n",
        "Using Laplacian Pyramid Deep Learning Networks (LAPIRN)\n",
        "\n",
        "This is a demo notebook for a github repository [LabIRN](https://github.com/cwmok/LapIRN/blob/master/Code/Train_LapIRN_disp.py). The original code author is [Tony Chi Wing MOK](https://cwmok.github.io/). If you have questions please open issue in the original repository. \n",
        "\n",
        "the paper can be downloaded from [here](https://arxiv.org/abs/2006.16148).\n",
        "\n",
        "\n",
        "The dataset used is from [learn2reg challenge](https://learn2reg.grand-challenge.org/Datasets). It can be downloaded from ()\n",
        "\n",
        "Notes:\n",
        "  - some bugs are fixed.\n",
        "\n",
        "Todos:\n",
        "  - use tensorboard to draw training and testing curves \n",
        "  - improve the reading/writing procedures\n",
        "  - remove redundant code\n",
        "\n",
        "\n",
        "**This notebook is controbuted by:** Ibraheem Al-Dhamari\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_be7w-AhOUi"
      },
      "source": [
        "# LapIRN: Large Deformation Diffeomorphic Image Registration with Laplacian Pyramid Networks \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeRnApb1hZf9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of **deformable registration** is computing a transformation to align a pair of images by minimizing a similarity metric that maximizes the similarity between the images. Usually, we call these images the fixed (or reference) image and the moving (or target) image. \n",
        "\n",
        "Deformable registration has many important applications and considered one of the most challenging problems. The general image registration problem is not solved despite many published papers every year trying to solve a special case of this general registration problem.  \n",
        "\n",
        "The **multi-resolution strategy** estimates the target transformation iteratively along number of levels. Each level estimates a transformation of a higher level. This usually produces faster computation and better results. \n",
        "\n",
        "One disadvantage of the conventional image registration methods is the large time required to complete the task. **Deep learning networks (DNN)** requires much less time to accomplish such task. \n",
        "\n",
        "In DNN, the image registration problem can be represented as pixel-wise image translation problem. The network learns the pixel-wise spatial correspondence of a pair of images using convolution. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCpO3ao7uulY"
      },
      "source": [
        "## Method\n",
        "\n",
        "Given a fixed 3D image $F$ and a moving 3D image $M$, the learning function $f$ computes a deformation field $\\phi$ using the learning parameters $\\theta$: \n",
        "\n",
        "$$\n",
        " f_{\\theta}(F,M,\\phi_{L-1},\\theta_{L-1})=\\phi\n",
        "$$\n",
        "\n",
        "This will be repeated for number of levels e.g. for three levels of multi-resolution $L=3$:\n",
        "\n",
        "$$\n",
        " f_{\\theta_3}(F_3,M_3)=\\phi_3 \\\\\n",
        " f_{\\theta_2}(F_2,M_2,\\phi_3,\\theta_3)=\\phi_2 \\\\\n",
        " f_{\\theta}(F,M,\\phi_2,\\theta_2)=\\phi \\\\\n",
        "$$\n",
        "\n",
        "In general:\n",
        "\n",
        "$$\n",
        " f_{\\theta_L}(F_L,M_L,\\phi_{L-1},\\theta_{L-1})=\\phi_L \\\\\n",
        "$$\n",
        "\n",
        "where the size of $F_L$ is the size of F / ${2^{L-1}}$. In other words: we get as maller size image by half at each level. In the final level, we use the original image size.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeyPpj9TzIHU"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "\n",
        "a L-level Laplacian pyramid framework to mimic the conventional multi-resolution strategy. For simplicity, we set L to 3 throughout this paper. The overview of LapIRN is illustrated in Fig. 1. Specifically, we first create the input image pyramid by downsampling the input images with trilinear interpolation to obtain Fi ∈ {F1, F2, F3} (and Mi ∈ {M1, M2, M3}), where Fi denotes the downsampled F with a scale factor 0.5(L−i) and F3 = F. We employ a CNN-based registration network (CRN) to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIvFGWlcGwla"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8QZwliIHDU",
        "outputId": "34b5f386-800d-4498-9ad1-61cca4af7d92"
      },
      "source": [
        "# It is better to work with google drive as downloading is not working!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W_x8y2tkiU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d5c515-15bc-4b65-f851-47e92d671076"
      },
      "source": [
        "\n",
        "installTools    = 1 # every time you restart colab virtual machine\n",
        "cloneGitRep     = 0 # one time thing, it will be saved in google drive\n",
        "downloadDataset = 0 # one time thing, it will be saved in google drive\n",
        "if installTools:\n",
        "    !pip3 install -U scikit-learn\n",
        "    !pip3 install simpleitk\n",
        "    !pip3 install tensorflow==1.14 tensorflow-gpu==1.14 keras==2.3.1\n",
        "    !pip3 install nibabel tqdm\n",
        "    !pip3 install torchvision torch==1.3.0 \n",
        "    print(\"installing tools is done!\")\n",
        "\n",
        "if cloneGitRep :   \n",
        "    #clone the updated code\n",
        "    !ls\n",
        "    wdPath = 'drive/MyDrive/LapIRN_org'\n",
        "    !mkdir $wdPath\n",
        "    !ls drive/MyDrive\n",
        "    !git clone https://github.com/idhamari/LapIRN.git $wdPath/LapIRN\n",
        "    print('-------------------')\n",
        "    !ls  $wdPath/LapIRN\n",
        "    print(\"cloning repository is done!\")\n",
        "\n",
        "if downloadDataset:    \n",
        "    # download the dataset    \n",
        "    !mkdir $wdPath/datasets\n",
        "    # note use the same lines with changing the only the id\n",
        "    !curl -c /tmp/cookies \"https://drive.google.com/uc?export=download&id=17uysjRAiXMIT2QApW5kHWP1aHCi5_lPO\" > tmp.txt\n",
        "    !curl -L -b /tmp/cookies \"https://drive.google.com$(cat tmp.txt | grep -Po 'uc-download-link\" [^>]* href=\"\\K[^\"]*' | sed 's/\\&amp;/\\&/g')\" >  $wdPath/datasets/L2R_Task3_AbdominalCT_160x192x144.zip\n",
        "    !ls  $wdPath/datasets/L2R_Task3_AbdominalCT_160x192x144.zip -l --block-size=M \n",
        "    !unzip $wdPath/datasets/L2R_Task3_AbdominalCT_160x192x144.zip -d $wdPath/datasets \n",
        "    !ls   $wdPath/datasets\n",
        "    print(\"downloading dataset is done!\")\n",
        "    firstRun = 0\n",
        "\n",
        "!ls\n",
        "\n",
        "import time , shutil, os\n",
        "from google.colab import files\n",
        "%matplotlib inline\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # first gpu\n",
        "\n",
        "# print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "# print(\"      important note: copy or download the results if you want to save them\")\n",
        "# print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
            "Collecting simpleitk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/6b/85df5eb3a8059b23a53a9f224476e75473f9bcc0a8583ed1a9c34619f372/SimpleITK-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 64kB/s \n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.0.2\n",
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3MB 38kB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/67/559ca8408431c37ad3a17e859c8c291ea82f092354074baef482b98ffb7b/tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1MB 44kB/s \n",
            "\u001b[?25hCollecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.12.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow, tensorflow-gpu, keras\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/0b/9d33aef363b6728ad937643d98be713c6c25d50ce338678ad57cee6e6fd5/torch-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.3.0\n",
            "installing tools is done!\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQtPocugG0eE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZnWL1uk6xfW",
        "outputId": "476dbbf5-8c7e-41fc-8218-cbe80ca1104c"
      },
      "source": [
        "doTraining = 1\n",
        "if doTraining:\n",
        "    import time , shutil, os\n",
        "    from google.colab import files\n",
        "    %matplotlib inline\n",
        "\n",
        "    !ls\n",
        "    # work_directory\n",
        "    wdPath = 'drive/MyDrive/LapIRN_org/'\n",
        "    # # dataset\n",
        "    dataset_path = '../../datasets/L2R_Task3_AbdominalCT_160x192x144'\n",
        "    # # 30 images,  image size =  192,160,256, each has its segmentation, the segmentation has 13 segmentation classes,\n",
        "    # images are already resized to fit the GPU memory \n",
        "\n",
        "    # remove old training files:\n",
        "\n",
        "    scriptPath = wdPath + 'LapIRN/demoLapIRN_org.py'\n",
        "    doTrain     = \" 1 \"\n",
        "    isLocal     = \" 0 \" # use gitlab or local workstation\n",
        "    slvl1       = \" 3000 \"  # start epoch lvl1    \n",
        "    slvl2       = \" 3000 \"  # start epoch lvl2       \n",
        "    slvl3       = \" 0 \"  # start epoch lvl3     \n",
        "    lvl1        = \" 3000 \"   # number of iterations for first resolution  \n",
        "    lvl2        = \" 3000 \"  # number of iterations for second resolution  \n",
        "    lvl3        = \" 30000 \" # number of iterations for third resolution \n",
        "    checkpoint  = \" 10 \"# model will be saved after each checkpoint steps    \n",
        "\n",
        "    if not ( (int(slvl1)>0) or (int(slvl2)>0) or (int(slvl3)>0)) :\n",
        "       if os.path.isdir(wdPath + 'LapIRN/Model/Stage'):\n",
        "           print(\"removing old stage files .......................................\")\n",
        "           shutil.rmtree(wdPath + 'LapIRN/Model/Stage')\n",
        "       else: \n",
        "          print(\"folder not found ..... \"+wdPath + 'LapIRN/Model/Stage')    \n",
        "\n",
        "\n",
        "    wdPath = '/content/drive/MyDrive/LapIRN_org/'\n",
        "    #!python $scriptPath $doTrain $slvl1 $slvl2 $slvl3 $lvl1 $lvl2 $lvl3 $checkpoint $wd_path $dataset_path\n",
        "    # cmd = \"python \" + scriptPath + \" \" + doTrain + isLocal + slvl1 +slvl2 +slvl3 +lvl1 +lvl2 +lvl3 + checkpoint  + \" \" +wdPath  + \" \" +dataset_path\n",
        "    # os.system(cmd)\n",
        "    \n",
        "    !python $scriptPath $doTrain $isLocal $slvl1 $slvl2 $slvl3 $lvl1 $lvl2 $lvl3 $checkpoint  $wdPath  $dataset_path\n",
        "\n",
        "    #save data every one hour\n",
        "    # print(\"process started ....\")\n",
        "    # while 1:\n",
        "    #     # save results every some hours\n",
        "    #     if os.path.isfile('model_folder_compressed.zip'):\n",
        "    #         os.remove('model_folder_compressed.zip')\n",
        "    #     print(\"creating archive ...............\")         \n",
        "    #     time.sleep(60*60)\n",
        "    #     shutil.make_archive('model_folder_compressed', 'zip', '/content/LapIRN_org/LapIRN/Model')      \n",
        "    #     print(\"downloading archive ...............\")         \n",
        "    #     time.sleep(15*60)\n",
        "    #     files.download('model_folder_compressed.zip') \n",
        "    #     time.sleep(15*300)\n",
        "    #     \n",
        "    # # download the pretrained model\n",
        "    print(\" training done! ....................\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "                 Setup                                        \n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:From drive/MyDrive/LapIRN_org/LapIRN/demoLapIRN_org.py:61: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From drive/MyDrive/LapIRN_org/LapIRN/demoLapIRN_org.py:62: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From drive/MyDrive/LapIRN_org/LapIRN/demoLapIRN_org.py:62: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2021-04-20 17:38:03.297809: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-04-20 17:38:03.362848: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2021-04-20 17:38:03.363292: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bad80e1640 executing computations on platform Host. Devices:\n",
            "2021-04-20 17:38:03.363333: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2021-04-20 17:38:03.374497: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-20 17:38:03.533405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:03.535348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bad80e1100 executing computations on platform CUDA. Devices:\n",
            "2021-04-20 17:38:03.535388: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2021-04-20 17:38:03.536565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:03.537430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-20 17:38:03.572357: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-20 17:38:03.778360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-04-20 17:38:03.882600: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-04-20 17:38:03.946196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-04-20 17:38:04.201440: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-04-20 17:38:04.356886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-04-20 17:38:04.886256: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-20 17:38:04.886456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:04.887325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:04.888030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2021-04-20 17:38:04.891103: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-20 17:38:04.893661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-20 17:38:04.893710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2021-04-20 17:38:04.893728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2021-04-20 17:38:04.897235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:04.898173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-20 17:38:04.898899: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-20 17:38:04.898960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9152 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "user arguments:  ['drive/MyDrive/LapIRN_org/LapIRN/demoLapIRN_org.py', '1', '0', '3000', '3000', '0', '3000', '3000', '30000', '500', '/content/drive/MyDrive/LapIRN_org/', '../../datasets/L2R_Task3_AbdominalCT_160x192x144']\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "                 Parameters Setting                           \n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "['drive/MyDrive/LapIRN_org/LapIRN/demoLapIRN_org.py', '1', '0', '3000', '3000', '0', '3000', '3000', '30000', '500', '/content/drive/MyDrive/LapIRN_org/', '../../datasets/L2R_Task3_AbdominalCT_160x192x144']\n",
            "using user arguments .................\n",
            "1.14.0\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "                 Train                                        \n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "--------------------------------------------\n",
            "        Start Training                      \n",
            "--------------------------------------------\n",
            "using user arguments .................\n",
            "script is running locally...........\n",
            "wd_path: /content/drive/MyDrive/LapIRN_org/\n",
            "python3    Train_LapIRN_disp.py  --datapath ../../datasets/L2R_Task3_AbdominalCT_160x192x144 --lr 0.0001 --sIteration_lvl1 3000 --sIteration_lvl2 3000 --sIteration_lvl3 0 --iteration_lvl1 3000 --iteration_lvl2 3000 --iteration_lvl3 30000 --checkpoint 500\n",
            "n_checkpoint :  500\n",
            "datapath :  ../../datasets/L2R_Task3_AbdominalCT_160x192x144\n",
            "../../datasets/L2R_Task3_AbdominalCT_160x192x144\n",
            "len(names) :  30\n",
            "imgshape:  (160, 192, 144)\n",
            "Namespace(antifold=0.0, checkpoint=500, datapath='../../datasets/L2R_Task3_AbdominalCT_160x192x144', freeze_step=2000, iteration_lvl1=3000, iteration_lvl2=3000, iteration_lvl3=30000, lr=0.0001, sIteration_lvl1=3000, sIteration_lvl2=3000, sIteration_lvl3=0, smooth=1.0, start_channel=7)\n",
            "Training lvl1...\n",
            "Loading weight and loss :  ../Model/Stage/LDR_OASIS_NCC_unit_disp_add_reg_1_stagelvl1_3000.pth\n",
            "Training lvl2...\n",
            "Loading weight for model_lvl1... ../Model/Stage/LDR_OASIS_NCC_unit_disp_add_reg_1_stagelvl1_3000.pth\n",
            "Loading weight and loss :  ../Model/Stage/LDR_OASIS_NCC_unit_disp_add_reg_1_stagelvl2_3000.pth\n",
            "Training lvl3...\n",
            "Loading weight for model_lvl2... ../Model/Stage/LDR_OASIS_NCC_unit_disp_add_reg_1_stagelvl2_3000.pth\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "step \"0\" -> training loss \"8373.8281\" - sim_NCC \"-0.177351\" - Jdet \"393835.8750000000\" -smo \"8374.0059\"\n",
            "step \"1\" -> training loss \"7506.6562\" - sim_NCC \"-0.106611\" - Jdet \"309067.3750000000\" -smo \"7506.7627\"\n",
            "step \"2\" -> training loss \"6815.9033\" - sim_NCC \"-0.202938\" - Jdet \"283879.8750000000\" -smo \"6816.1064\"\n",
            "step \"3\" -> training loss \"6296.6631\" - sim_NCC \"-0.082942\" - Jdet \"246531.5937500000\" -smo \"6296.7461\"\n",
            "step \"4\" -> training loss \"5869.7695\" - sim_NCC \"-0.166993\" - Jdet \"192861.0625000000\" -smo \"5869.9365\"\n",
            "step \"5\" -> training loss \"5405.1187\" - sim_NCC \"-0.072479\" - Jdet \"161199.7968750000\" -smo \"5405.1909\"\n",
            "step \"6\" -> training loss \"5232.2153\" - sim_NCC \"-0.172438\" - Jdet \"145865.0781250000\" -smo \"5232.3877\"\n",
            "step \"7\" -> training loss \"4706.0566\" - sim_NCC \"-0.094513\" - Jdet \"92829.1250000000\" -smo \"4706.1514\"\n",
            "step \"8\" -> training loss \"3785.4934\" - sim_NCC \"-0.064800\" - Jdet \"26956.4199218750\" -smo \"3785.5581\"\n",
            "step \"9\" -> training loss \"3122.1069\" - sim_NCC \"-0.208069\" - Jdet \"4523.8403320312\" -smo \"3122.3149\"\n",
            "step \"10\" -> training loss \"2117.3032\" - sim_NCC \"-0.197255\" - Jdet \"1864.8114013672\" -smo \"2117.5005\"\n",
            "step \"11\" -> training loss \"1343.6125\" - sim_NCC \"-0.166570\" - Jdet \"1189.2437744141\" -smo \"1343.7792\"\n",
            "step \"12\" -> training loss \"1061.7751\" - sim_NCC \"-0.161246\" - Jdet \"221.6807861328\" -smo \"1061.9364\"\n",
            "step \"13\" -> training loss \"938.9922\" - sim_NCC \"-0.545724\" - Jdet \"81.9489593506\" -smo \"939.5380\"\n",
            "step \"14\" -> training loss \"903.7209\" - sim_NCC \"-0.196697\" - Jdet \"72.5030212402\" -smo \"903.9176\"\n",
            "step \"15\" -> training loss \"891.7075\" - sim_NCC \"-0.185889\" - Jdet \"72.6710281372\" -smo \"891.8934\"\n",
            "step \"16\" -> training loss \"868.8419\" - sim_NCC \"-0.267234\" - Jdet \"60.3607559204\" -smo \"869.1091\"\n",
            "step \"17\" -> training loss \"721.5316\" - sim_NCC \"-0.207310\" - Jdet \"55.0484085083\" -smo \"721.7389\"\n",
            "step \"18\" -> training loss \"623.6617\" - sim_NCC \"-0.350419\" - Jdet \"46.2107543945\" -smo \"624.0121\"\n",
            "step \"19\" -> training loss \"488.2360\" - sim_NCC \"-0.192292\" - Jdet \"24.5141010284\" -smo \"488.4283\"\n",
            "step \"20\" -> training loss \"406.5897\" - sim_NCC \"-0.197117\" - Jdet \"21.3535804749\" -smo \"406.7868\"\n",
            "step \"21\" -> training loss \"430.4049\" - sim_NCC \"-0.198982\" - Jdet \"21.2682647705\" -smo \"430.6039\"\n",
            "step \"22\" -> training loss \"444.4427\" - sim_NCC \"-0.115744\" - Jdet \"21.0417156219\" -smo \"444.5584\"\n",
            "step \"23\" -> training loss \"444.1660\" - sim_NCC \"-0.144871\" - Jdet \"21.1912231445\" -smo \"444.3109\"\n",
            "step \"24\" -> training loss \"437.1679\" - sim_NCC \"-0.250557\" - Jdet \"20.9002456665\" -smo \"437.4184\"\n",
            "step \"25\" -> training loss \"419.7920\" - sim_NCC \"-0.221173\" - Jdet \"21.4213047028\" -smo \"420.0132\"\n",
            "step \"26\" -> training loss \"404.6244\" - sim_NCC \"-0.249762\" - Jdet \"20.4271049500\" -smo \"404.8741\"\n",
            "step \"27\" -> training loss \"351.1618\" - sim_NCC \"-0.220059\" - Jdet \"20.3526115417\" -smo \"351.3818\"\n",
            "step \"28\" -> training loss \"315.5776\" - sim_NCC \"-0.134551\" - Jdet \"19.2233638763\" -smo \"315.7122\"\n",
            "step \"29\" -> training loss \"346.1193\" - sim_NCC \"-0.083950\" - Jdet \"19.6616401672\" -smo \"346.2032\"\n",
            "step \"30\" -> training loss \"355.6836\" - sim_NCC \"-0.218954\" - Jdet \"19.9158325195\" -smo \"355.9025\"\n",
            "step \"31\" -> training loss \"355.6010\" - sim_NCC \"-0.216142\" - Jdet \"20.0183315277\" -smo \"355.8171\"\n",
            "step \"32\" -> training loss \"353.9717\" - sim_NCC \"-0.206748\" - Jdet \"19.6769924164\" -smo \"354.1785\"\n",
            "step \"33\" -> training loss \"351.0619\" - sim_NCC \"-0.131183\" - Jdet \"19.7692222595\" -smo \"351.1931\"\n",
            "step \"34\" -> training loss \"345.0003\" - sim_NCC \"-0.193112\" - Jdet \"19.9144153595\" -smo \"345.1934\"\n",
            "step \"35\" -> training loss \"343.5536\" - sim_NCC \"-0.120479\" - Jdet \"19.4538726807\" -smo \"343.6741\"\n",
            "step \"36\" -> training loss \"339.4467\" - sim_NCC \"-0.251411\" - Jdet \"19.3340072632\" -smo \"339.6981\"\n",
            "step \"37\" -> training loss \"334.7200\" - sim_NCC \"-0.211858\" - Jdet \"19.6609363556\" -smo \"334.9319\"\n",
            "step \"38\" -> training loss \"334.2235\" - sim_NCC \"-0.187145\" - Jdet \"19.2315807343\" -smo \"334.4106\"\n",
            "step \"39\" -> training loss \"329.1100\" - sim_NCC \"-0.202319\" - Jdet \"19.4171028137\" -smo \"329.3123\"\n",
            "step \"40\" -> training loss \"328.2834\" - sim_NCC \"-0.196404\" - Jdet \"19.0134887695\" -smo \"328.4798\"\n",
            "step \"41\" -> training loss \"324.1093\" - sim_NCC \"-0.202886\" - Jdet \"18.9613609314\" -smo \"324.3121\"\n",
            "step \"42\" -> training loss \"321.8753\" - sim_NCC \"-0.190878\" - Jdet \"18.8663978577\" -smo \"322.0662\"\n",
            "step \"43\" -> training loss \"319.2157\" - sim_NCC \"-0.180193\" - Jdet \"18.8764476776\" -smo \"319.3959\"\n",
            "step \"44\" -> training loss \"316.2543\" - sim_NCC \"-0.276759\" - Jdet \"18.8129825592\" -smo \"316.5310\"\n",
            "step \"45\" -> training loss \"312.3540\" - sim_NCC \"-0.158418\" - Jdet \"18.8669700623\" -smo \"312.5125\"\n",
            "step \"46\" -> training loss \"311.6400\" - sim_NCC \"-0.198140\" - Jdet \"18.8238010406\" -smo \"311.8381\"\n",
            "step \"47\" -> training loss \"311.3253\" - sim_NCC \"-0.301663\" - Jdet \"18.6644554138\" -smo \"311.6270\"\n",
            "step \"48\" -> training loss \"309.3808\" - sim_NCC \"-1.216450\" - Jdet \"18.6239414215\" -smo \"310.5972\"\n",
            "step \"49\" -> training loss \"320.0906\" - sim_NCC \"-0.208442\" - Jdet \"18.8433609009\" -smo \"320.2990\"\n",
            "step \"50\" -> training loss \"336.4899\" - sim_NCC \"-0.185492\" - Jdet \"19.0548191071\" -smo \"336.6754\"\n",
            "step \"51\" -> training loss \"359.0280\" - sim_NCC \"-0.193958\" - Jdet \"19.3483982086\" -smo \"359.2219\"\n",
            "step \"52\" -> training loss \"448.3763\" - sim_NCC \"-0.324656\" - Jdet \"20.4824028015\" -smo \"448.7010\"\n",
            "step \"53\" -> training loss \"513.3659\" - sim_NCC \"-0.134399\" - Jdet \"21.4486579895\" -smo \"513.5003\"\n",
            "step \"54\" -> training loss \"523.4993\" - sim_NCC \"-0.253945\" - Jdet \"21.4680061340\" -smo \"523.7533\"\n",
            "step \"55\" -> training loss \"447.6002\" - sim_NCC \"-0.191524\" - Jdet \"20.5542411804\" -smo \"447.7917\"\n",
            "step \"56\" -> training loss \"374.3689\" - sim_NCC \"-0.192557\" - Jdet \"19.9961528778\" -smo \"374.5615\"\n",
            "step \"57\" -> training loss \"345.2778\" - sim_NCC \"-0.163691\" - Jdet \"19.1132488251\" -smo \"345.4415\"\n",
            "step \"58\" -> training loss \"334.8932\" - sim_NCC \"-0.209892\" - Jdet \"19.1166992188\" -smo \"335.1031\"\n",
            "step \"59\" -> training loss \"329.9019\" - sim_NCC \"-0.166069\" - Jdet \"19.1625823975\" -smo \"330.0680\"\n",
            "step \"60\" -> training loss \"325.8690\" - sim_NCC \"-0.080375\" - Jdet \"19.1562213898\" -smo \"325.9494\"\n",
            "step \"61\" -> training loss \"325.2273\" - sim_NCC \"-0.199124\" - Jdet \"18.7378711700\" -smo \"325.4265\"\n",
            "step \"62\" -> training loss \"323.5774\" - sim_NCC \"-0.179502\" - Jdet \"18.6477565765\" -smo \"323.7569\"\n",
            "step \"63\" -> training loss \"320.9106\" - sim_NCC \"-0.710649\" - Jdet \"18.5510845184\" -smo \"321.6212\"\n",
            "step \"64\" -> training loss \"318.2157\" - sim_NCC \"-0.197878\" - Jdet \"18.5317974091\" -smo \"318.4136\"\n",
            "step \"65\" -> training loss \"316.7348\" - sim_NCC \"-0.180174\" - Jdet \"18.5029067993\" -smo \"316.9150\"\n",
            "step \"66\" -> training loss \"313.8281\" - sim_NCC \"-0.189815\" - Jdet \"18.6090240479\" -smo \"314.0179\"\n",
            "step \"67\" -> training loss \"314.0539\" - sim_NCC \"-0.202068\" - Jdet \"18.4410152435\" -smo \"314.2559\"\n",
            "step \"68\" -> training loss \"312.7975\" - sim_NCC \"-0.314936\" - Jdet \"18.3633251190\" -smo \"313.1124\"\n",
            "step \"69\" -> training loss \"311.1716\" - sim_NCC \"-0.162190\" - Jdet \"18.4224910736\" -smo \"311.3338\"\n",
            "step \"70\" -> training loss \"310.7337\" - sim_NCC \"-0.080155\" - Jdet \"18.4754314423\" -smo \"310.8138\"\n",
            "step \"71\" -> training loss \"308.9725\" - sim_NCC \"-0.190586\" - Jdet \"18.3415088654\" -smo \"309.1631\"\n",
            "step \"72\" -> training loss \"306.4874\" - sim_NCC \"-0.202906\" - Jdet \"18.2988376617\" -smo \"306.6903\"\n",
            "step \"73\" -> training loss \"307.6718\" - sim_NCC \"-0.194421\" - Jdet \"18.3274383545\" -smo \"307.8662\"\n",
            "step \"74\" -> training loss \"303.4380\" - sim_NCC \"-0.211875\" - Jdet \"18.3030052185\" -smo \"303.6499\"\n",
            "step \"75\" -> training loss \"305.3261\" - sim_NCC \"-0.084643\" - Jdet \"18.2517070770\" -smo \"305.4107\"\n",
            "step \"76\" -> training loss \"301.0577\" - sim_NCC \"-0.219899\" - Jdet \"18.1297149658\" -smo \"301.2776\"\n",
            "step \"77\" -> training loss \"299.7206\" - sim_NCC \"-0.189510\" - Jdet \"18.0405731201\" -smo \"299.9102\"\n",
            "step \"78\" -> training loss \"299.9762\" - sim_NCC \"-0.828179\" - Jdet \"17.9842548370\" -smo \"300.8044\"\n",
            "step \"79\" -> training loss \"298.3279\" - sim_NCC \"-0.188988\" - Jdet \"17.9435977936\" -smo \"298.5169\"\n",
            "step \"80\" -> training loss \"296.6635\" - sim_NCC \"-0.197738\" - Jdet \"17.8457736969\" -smo \"296.8612\"\n",
            "step \"81\" -> training loss \"300.4192\" - sim_NCC \"-0.276998\" - Jdet \"17.8737869263\" -smo \"300.6962\"\n",
            "step \"82\" -> training loss \"296.4362\" - sim_NCC \"-0.208423\" - Jdet \"17.8059196472\" -smo \"296.6446\"\n",
            "step \"83\" -> training loss \"298.2232\" - sim_NCC \"-0.202741\" - Jdet \"17.8085384369\" -smo \"298.4260\"\n",
            "step \"84\" -> training loss \"297.8575\" - sim_NCC \"-0.205124\" - Jdet \"17.8079719543\" -smo \"298.0626\"\n",
            "step \"85\" -> training loss \"296.8105\" - sim_NCC \"-0.224370\" - Jdet \"17.6876258850\" -smo \"297.0349\"\n",
            "step \"86\" -> training loss \"299.3683\" - sim_NCC \"-0.246786\" - Jdet \"17.6592369080\" -smo \"299.6151\"\n",
            "step \"87\" -> training loss \"298.7631\" - sim_NCC \"-0.209377\" - Jdet \"17.6447334290\" -smo \"298.9725\"\n",
            "step \"88\" -> training loss \"298.0026\" - sim_NCC \"-0.243943\" - Jdet \"17.6355953217\" -smo \"298.2466\"\n",
            "step \"89\" -> training loss \"293.0714\" - sim_NCC \"-0.217020\" - Jdet \"17.4743404388\" -smo \"293.2884\"\n",
            "step \"90\" -> training loss \"295.0653\" - sim_NCC \"-0.801185\" - Jdet \"17.5443782806\" -smo \"295.8665\"\n",
            "step \"91\" -> training loss \"294.4253\" - sim_NCC \"-0.215098\" - Jdet \"17.5811138153\" -smo \"294.6404\"\n",
            "step \"92\" -> training loss \"292.4394\" - sim_NCC \"-0.193860\" - Jdet \"17.4712142944\" -smo \"292.6332\"\n",
            "step \"93\" -> training loss \"291.5867\" - sim_NCC \"-0.252135\" - Jdet \"17.4330139160\" -smo \"291.8389\"\n",
            "step \"94\" -> training loss \"290.0836\" - sim_NCC \"-0.291585\" - Jdet \"17.3999195099\" -smo \"290.3752\"\n",
            "step \"95\" -> training loss \"282.9294\" - sim_NCC \"-0.199474\" - Jdet \"17.1281261444\" -smo \"283.1288\"\n",
            "step \"96\" -> training loss \"283.9124\" - sim_NCC \"-0.191701\" - Jdet \"17.3621406555\" -smo \"284.1041\"\n",
            "step \"97\" -> training loss \"283.1330\" - sim_NCC \"-0.224785\" - Jdet \"17.2846622467\" -smo \"283.3578\"\n",
            "step \"98\" -> training loss \"279.6943\" - sim_NCC \"-0.175782\" - Jdet \"17.2323741913\" -smo \"279.8701\"\n",
            "step \"99\" -> training loss \"274.5602\" - sim_NCC \"-0.204396\" - Jdet \"17.0728397369\" -smo \"274.7646\"\n",
            "step \"100\" -> training loss \"263.9681\" - sim_NCC \"-0.219803\" - Jdet \"16.7214431763\" -smo \"264.1879\"\n",
            "step \"101\" -> training loss \"263.9484\" - sim_NCC \"-0.063301\" - Jdet \"17.0660018921\" -smo \"264.0117\"\n",
            "step \"102\" -> training loss \"246.7333\" - sim_NCC \"-0.206828\" - Jdet \"16.6211547852\" -smo \"246.9401\"\n",
            "step \"103\" -> training loss \"219.1531\" - sim_NCC \"-0.092560\" - Jdet \"16.1177425385\" -smo \"219.2456\"\n",
            "step \"104\" -> training loss \"192.3015\" - sim_NCC \"-0.110328\" - Jdet \"15.6755599976\" -smo \"192.4118\"\n",
            "step \"105\" -> training loss \"172.2618\" - sim_NCC \"-0.192573\" - Jdet \"15.4112911224\" -smo \"172.4544\"\n",
            "step \"106\" -> training loss \"157.4036\" - sim_NCC \"-0.089962\" - Jdet \"15.2441377640\" -smo \"157.4936\"\n",
            "step \"107\" -> training loss \"137.6259\" - sim_NCC \"-0.214138\" - Jdet \"14.2834844589\" -smo \"137.8401\"\n",
            "step \"108\" -> training loss \"117.1418\" - sim_NCC \"-0.089891\" - Jdet \"13.8555164337\" -smo \"117.2317\"\n",
            "step \"109\" -> training loss \"94.8298\" - sim_NCC \"-0.196976\" - Jdet \"13.5074033737\" -smo \"95.0268\"\n",
            "step \"110\" -> training loss \"78.2475\" - sim_NCC \"-0.216095\" - Jdet \"12.7554168701\" -smo \"78.4636\"\n",
            "step \"111\" -> training loss \"64.6541\" - sim_NCC \"-0.239743\" - Jdet \"12.4208698273\" -smo \"64.8939\"\n",
            "step \"112\" -> training loss \"54.3302\" - sim_NCC \"-0.233533\" - Jdet \"11.9477539062\" -smo \"54.5637\"\n",
            "step \"113\" -> training loss \"45.5584\" - sim_NCC \"-0.197318\" - Jdet \"11.2985792160\" -smo \"45.7557\"\n",
            "step \"114\" -> training loss \"37.9148\" - sim_NCC \"-0.202243\" - Jdet \"10.6272325516\" -smo \"38.1170\"\n",
            "step \"115\" -> training loss \"32.9445\" - sim_NCC \"-0.238686\" - Jdet \"9.9243516922\" -smo \"33.1831\"\n",
            "step \"116\" -> training loss \"28.7905\" - sim_NCC \"-0.221824\" - Jdet \"9.0821704865\" -smo \"29.0123\"\n",
            "step \"117\" -> training loss \"25.4756\" - sim_NCC \"-0.215608\" - Jdet \"8.4990205765\" -smo \"25.6913\"\n",
            "step \"118\" -> training loss \"23.3921\" - sim_NCC \"-0.064666\" - Jdet \"8.0895814896\" -smo \"23.4568\"\n",
            "step \"119\" -> training loss \"21.1612\" - sim_NCC \"-0.724986\" - Jdet \"7.7385301590\" -smo \"21.8861\"\n",
            "step \"120\" -> training loss \"19.8694\" - sim_NCC \"-0.229410\" - Jdet \"7.1761837006\" -smo \"20.0988\"\n",
            "step \"121\" -> training loss \"18.0516\" - sim_NCC \"-0.235526\" - Jdet \"6.6873331070\" -smo \"18.2871\"\n",
            "step \"122\" -> training loss \"15.3222\" - sim_NCC \"-0.131613\" - Jdet \"4.6815996170\" -smo \"15.4538\"\n",
            "step \"123\" -> training loss \"12.3963\" - sim_NCC \"-0.095862\" - Jdet \"1.0079294443\" -smo \"12.4922\"\n",
            "step \"124\" -> training loss \"11.5448\" - sim_NCC \"-0.242034\" - Jdet \"0.3435271084\" -smo \"11.7868\"\n",
            "step \"125\" -> training loss \"10.4534\" - sim_NCC \"-0.107033\" - Jdet \"0.2713183761\" -smo \"10.5605\"\n",
            "step \"126\" -> training loss \"7.2161\" - sim_NCC \"-0.197141\" - Jdet \"0.2325061411\" -smo \"7.4132\"\n",
            "step \"127\" -> training loss \"5.1350\" - sim_NCC \"-0.235560\" - Jdet \"0.1831253022\" -smo \"5.3706\"\n",
            "step \"128\" -> training loss \"3.1625\" - sim_NCC \"-0.824643\" - Jdet \"0.1530425549\" -smo \"3.9872\"\n",
            "step \"129\" -> training loss \"3.1869\" - sim_NCC \"-0.295762\" - Jdet \"0.1139508486\" -smo \"3.4827\"\n",
            "step \"130\" -> training loss \"1.4312\" - sim_NCC \"-1.741049\" - Jdet \"0.0532433800\" -smo \"3.1722\"\n",
            "step \"131\" -> training loss \"2.7557\" - sim_NCC \"-0.221965\" - Jdet \"0.0808030292\" -smo \"2.9777\"\n",
            "step \"132\" -> training loss \"2.5130\" - sim_NCC \"-0.219237\" - Jdet \"0.0487256683\" -smo \"2.7322\"\n",
            "step \"133\" -> training loss \"2.2734\" - sim_NCC \"-0.215354\" - Jdet \"0.1085524485\" -smo \"2.4888\"\n",
            "step \"134\" -> training loss \"2.1969\" - sim_NCC \"-0.253007\" - Jdet \"0.0286238324\" -smo \"2.4499\"\n",
            "step \"135\" -> training loss \"2.1991\" - sim_NCC \"-0.189954\" - Jdet \"0.0488588251\" -smo \"2.3891\"\n",
            "step \"136\" -> training loss \"2.1329\" - sim_NCC \"-0.167248\" - Jdet \"0.0498378426\" -smo \"2.3001\"\n",
            "step \"137\" -> training loss \"2.0030\" - sim_NCC \"-0.205729\" - Jdet \"0.0664332509\" -smo \"2.2087\"\n",
            "step \"138\" -> training loss \"1.9065\" - sim_NCC \"-0.227816\" - Jdet \"0.0303193796\" -smo \"2.1343\"\n",
            "step \"139\" -> training loss \"1.8673\" - sim_NCC \"-0.204947\" - Jdet \"0.0281829964\" -smo \"2.0722\"\n",
            "step \"140\" -> training loss \"1.7885\" - sim_NCC \"-0.232033\" - Jdet \"0.0250112507\" -smo \"2.0206\"\n",
            "step \"141\" -> training loss \"1.3688\" - sim_NCC \"-0.652015\" - Jdet \"0.0165589489\" -smo \"2.0208\"\n",
            "step \"142\" -> training loss \"1.7873\" - sim_NCC \"-0.197396\" - Jdet \"0.0309545640\" -smo \"1.9847\"\n",
            "step \"143\" -> training loss \"1.4192\" - sim_NCC \"-0.541764\" - Jdet \"0.0179744158\" -smo \"1.9609\"\n",
            "step \"144\" -> training loss \"1.6905\" - sim_NCC \"-0.204019\" - Jdet \"0.0425635464\" -smo \"1.8945\"\n",
            "step \"145\" -> training loss \"1.6408\" - sim_NCC \"-0.270986\" - Jdet \"0.0202547852\" -smo \"1.9118\"\n",
            "step \"146\" -> training loss \"-0.9487\" - sim_NCC \"-2.851927\" - Jdet \"0.0189834423\" -smo \"1.9033\"\n",
            "step \"147\" -> training loss \"1.6244\" - sim_NCC \"-0.259899\" - Jdet \"0.0182245541\" -smo \"1.8843\"\n",
            "step \"148\" -> training loss \"1.5626\" - sim_NCC \"-0.203289\" - Jdet \"0.0686289743\" -smo \"1.7659\"\n",
            "step \"149\" -> training loss \"1.6172\" - sim_NCC \"-0.223829\" - Jdet \"0.0640228391\" -smo \"1.8410\"\n",
            "step \"150\" -> training loss \"1.6232\" - sim_NCC \"-0.194424\" - Jdet \"0.0879049227\" -smo \"1.8176\"\n",
            "step \"151\" -> training loss \"1.6936\" - sim_NCC \"-0.232091\" - Jdet \"0.0215112790\" -smo \"1.9257\"\n",
            "step \"152\" -> training loss \"1.6828\" - sim_NCC \"-0.171865\" - Jdet \"0.0602575094\" -smo \"1.8547\"\n",
            "step \"153\" -> training loss \"1.6826\" - sim_NCC \"-0.194897\" - Jdet \"0.0472947620\" -smo \"1.8775\"\n",
            "step \"154\" -> training loss \"1.6502\" - sim_NCC \"-0.215672\" - Jdet \"0.0488088354\" -smo \"1.8659\"\n",
            "step \"155\" -> training loss \"1.7183\" - sim_NCC \"-0.180827\" - Jdet \"0.0439590514\" -smo \"1.8992\"\n",
            "step \"156\" -> training loss \"1.6597\" - sim_NCC \"-0.206975\" - Jdet \"0.0482417494\" -smo \"1.8666\"\n",
            "step \"157\" -> training loss \"1.7867\" - sim_NCC \"-0.092626\" - Jdet \"0.0739112198\" -smo \"1.8793\"\n",
            "step \"158\" -> training loss \"1.7219\" - sim_NCC \"-0.264385\" - Jdet \"0.0182773173\" -smo \"1.9863\"\n",
            "step \"159\" -> training loss \"1.7567\" - sim_NCC \"-0.259322\" - Jdet \"0.0153635852\" -smo \"2.0160\"\n",
            "step \"160\" -> training loss \"1.6262\" - sim_NCC \"-0.235631\" - Jdet \"0.0544713847\" -smo \"1.8618\"\n",
            "step \"161\" -> training loss \"1.7285\" - sim_NCC \"-0.241729\" - Jdet \"0.0272147171\" -smo \"1.9702\"\n",
            "step \"162\" -> training loss \"1.2657\" - sim_NCC \"-0.712518\" - Jdet \"0.0187802371\" -smo \"1.9782\"\n",
            "step \"163\" -> training loss \"1.7437\" - sim_NCC \"-0.222931\" - Jdet \"0.0267273616\" -smo \"1.9666\"\n",
            "step \"164\" -> training loss \"1.9049\" - sim_NCC \"-0.070103\" - Jdet \"0.0336340889\" -smo \"1.9750\"\n",
            "step \"165\" -> training loss \"1.6725\" - sim_NCC \"-0.180164\" - Jdet \"0.0633583367\" -smo \"1.8526\"\n",
            "step \"166\" -> training loss \"1.7244\" - sim_NCC \"-0.190207\" - Jdet \"0.0382337719\" -smo \"1.9146\"\n",
            "step \"167\" -> training loss \"1.7660\" - sim_NCC \"-0.217018\" - Jdet \"0.0220474117\" -smo \"1.9830\"\n",
            "step \"168\" -> training loss \"1.8431\" - sim_NCC \"-0.148422\" - Jdet \"0.0217080135\" -smo \"1.9916\"\n",
            "step \"169\" -> training loss \"1.6250\" - sim_NCC \"-0.221741\" - Jdet \"0.0608598739\" -smo \"1.8467\"\n",
            "step \"170\" -> training loss \"1.5917\" - sim_NCC \"-0.386553\" - Jdet \"0.0177918784\" -smo \"1.9782\"\n",
            "step \"171\" -> training loss \"1.7416\" - sim_NCC \"-0.245339\" - Jdet \"0.0173702706\" -smo \"1.9869\"\n",
            "step \"172\" -> training loss \"1.6869\" - sim_NCC \"-0.221755\" - Jdet \"0.0305188205\" -smo \"1.9086\"\n",
            "step \"173\" -> training loss \"1.7171\" - sim_NCC \"-0.220247\" - Jdet \"0.0206322558\" -smo \"1.9374\"\n",
            "step \"174\" -> training loss \"1.6181\" - sim_NCC \"-0.231089\" - Jdet \"0.0417198613\" -smo \"1.8492\"\n",
            "step \"175\" -> training loss \"1.6942\" - sim_NCC \"-0.216973\" - Jdet \"0.0209290255\" -smo \"1.9112\"\n",
            "step \"176\" -> training loss \"1.6130\" - sim_NCC \"-0.171499\" - Jdet \"0.0618578978\" -smo \"1.7845\"\n",
            "step \"177\" -> training loss \"1.6898\" - sim_NCC \"-0.120112\" - Jdet \"0.0520483255\" -smo \"1.8099\"\n",
            "step \"178\" -> training loss \"1.6889\" - sim_NCC \"-0.185397\" - Jdet \"0.0357893594\" -smo \"1.8743\"\n",
            "step \"179\" -> training loss \"1.6779\" - sim_NCC \"-0.091206\" - Jdet \"0.0725347176\" -smo \"1.7691\"\n",
            "step \"180\" -> training loss \"1.7268\" - sim_NCC \"-0.144039\" - Jdet \"0.0316304117\" -smo \"1.8708\"\n",
            "step \"181\" -> training loss \"1.5975\" - sim_NCC \"-0.226829\" - Jdet \"0.0380404666\" -smo \"1.8243\"\n",
            "step \"182\" -> training loss \"1.6767\" - sim_NCC \"-0.093795\" - Jdet \"0.0834145322\" -smo \"1.7705\"\n",
            "step \"183\" -> training loss \"1.6021\" - sim_NCC \"-0.169468\" - Jdet \"0.0479393974\" -smo \"1.7716\"\n",
            "step \"184\" -> training loss \"-0.2126\" - sim_NCC \"-2.075751\" - Jdet \"0.0161365438\" -smo \"1.8631\"\n",
            "step \"185\" -> training loss \"1.6410\" - sim_NCC \"-0.169175\" - Jdet \"0.0379196107\" -smo \"1.8102\"\n",
            "step \"186\" -> training loss \"1.6419\" - sim_NCC \"-0.214384\" - Jdet \"0.0248652231\" -smo \"1.8562\"\n",
            "step \"187\" -> training loss \"-1.4140\" - sim_NCC \"-3.282296\" - Jdet \"0.0166943800\" -smo \"1.8683\"\n",
            "step \"188\" -> training loss \"1.6398\" - sim_NCC \"-0.205758\" - Jdet \"0.0304510351\" -smo \"1.8456\"\n",
            "step \"189\" -> training loss \"1.5698\" - sim_NCC \"-0.219846\" - Jdet \"0.0492251255\" -smo \"1.7896\"\n",
            "step \"190\" -> training loss \"1.6389\" - sim_NCC \"-0.191847\" - Jdet \"0.0579775646\" -smo \"1.8308\"\n",
            "step \"191\" -> training loss \"1.7842\" - sim_NCC \"-0.205655\" - Jdet \"0.0213414710\" -smo \"1.9899\"\n",
            "step \"192\" -> training loss \"0.1307\" - sim_NCC \"-1.912987\" - Jdet \"0.0151667399\" -smo \"2.0437\"\n",
            "step \"193\" -> training loss \"1.7852\" - sim_NCC \"-0.229866\" - Jdet \"0.0549579635\" -smo \"2.0150\"\n",
            "step \"194\" -> training loss \"2.0324\" - sim_NCC \"-0.164377\" - Jdet \"0.0412234291\" -smo \"2.1968\"\n",
            "step \"195\" -> training loss \"2.0197\" - sim_NCC \"-0.237327\" - Jdet \"0.0429197848\" -smo \"2.2570\"\n",
            "step \"196\" -> training loss \"2.1386\" - sim_NCC \"-0.345640\" - Jdet \"0.0214190464\" -smo \"2.4842\"\n",
            "step \"197\" -> training loss \"2.2509\" - sim_NCC \"-0.235879\" - Jdet \"0.0255274028\" -smo \"2.4868\"\n",
            "step \"198\" -> training loss \"2.2367\" - sim_NCC \"-0.240713\" - Jdet \"0.0427998565\" -smo \"2.4775\"\n",
            "step \"199\" -> training loss \"2.2040\" - sim_NCC \"-0.214906\" - Jdet \"0.0637691543\" -smo \"2.4189\"\n",
            "step \"200\" -> training loss \"2.5651\" - sim_NCC \"-0.082274\" - Jdet \"0.0445033684\" -smo \"2.6474\"\n",
            "step \"201\" -> training loss \"2.5843\" - sim_NCC \"-0.173269\" - Jdet \"0.0239163171\" -smo \"2.7576\"\n",
            "step \"202\" -> training loss \"1.5793\" - sim_NCC \"-1.223141\" - Jdet \"0.0211331788\" -smo \"2.8024\"\n",
            "step \"203\" -> training loss \"2.6358\" - sim_NCC \"-0.274528\" - Jdet \"0.0207963083\" -smo \"2.9104\"\n",
            "step \"204\" -> training loss \"2.7603\" - sim_NCC \"-0.222817\" - Jdet \"0.0350235440\" -smo \"2.9831\"\n",
            "step \"205\" -> training loss \"2.8393\" - sim_NCC \"-0.214696\" - Jdet \"0.0689182132\" -smo \"3.0540\"\n",
            "step \"206\" -> training loss \"3.0552\" - sim_NCC \"-0.181971\" - Jdet \"0.0380592532\" -smo \"3.2371\"\n",
            "step \"207\" -> training loss \"3.0603\" - sim_NCC \"-0.232307\" - Jdet \"0.0348226950\" -smo \"3.2927\"\n",
            "step \"208\" -> training loss \"2.5722\" - sim_NCC \"-0.892286\" - Jdet \"0.0273027550\" -smo \"3.4645\"\n",
            "step \"209\" -> training loss \"3.3149\" - sim_NCC \"-0.207362\" - Jdet \"0.0445231460\" -smo \"3.5223\"\n",
            "step \"210\" -> training loss \"3.5374\" - sim_NCC \"-0.100957\" - Jdet \"0.0379800871\" -smo \"3.6383\"\n",
            "step \"211\" -> training loss \"3.3187\" - sim_NCC \"-0.225755\" - Jdet \"0.0895655900\" -smo \"3.5445\"\n",
            "step \"212\" -> training loss \"1.9166\" - sim_NCC \"-1.803933\" - Jdet \"0.0285210963\" -smo \"3.7205\"\n",
            "step \"213\" -> training loss \"3.6372\" - sim_NCC \"-0.221093\" - Jdet \"0.0497011580\" -smo \"3.8583\"\n",
            "step \"214\" -> training loss \"3.4850\" - sim_NCC \"-0.466602\" - Jdet \"0.0306655057\" -smo \"3.9516\"\n",
            "step \"215\" -> training loss \"3.8324\" - sim_NCC \"-0.213877\" - Jdet \"0.0545466393\" -smo \"4.0463\"\n",
            "step \"216\" -> training loss \"3.9244\" - sim_NCC \"-0.230194\" - Jdet \"0.0511063151\" -smo \"4.1546\"\n",
            "step \"217\" -> training loss \"2.9943\" - sim_NCC \"-1.160274\" - Jdet \"0.0345753953\" -smo \"4.1546\"\n",
            "step \"218\" -> training loss \"4.2125\" - sim_NCC \"-0.099063\" - Jdet \"0.0337834023\" -smo \"4.3116\"\n",
            "step \"219\" -> training loss \"4.0541\" - sim_NCC \"-0.229529\" - Jdet \"0.0757830217\" -smo \"4.2837\"\n",
            "step \"220\" -> training loss \"3.8878\" - sim_NCC \"-0.183449\" - Jdet \"0.0759873539\" -smo \"4.0713\"\n",
            "step \"221\" -> training loss \"4.0646\" - sim_NCC \"-0.207220\" - Jdet \"0.0637663826\" -smo \"4.2718\"\n",
            "step \"222\" -> training loss \"4.1021\" - sim_NCC \"-0.087047\" - Jdet \"0.0944024324\" -smo \"4.1892\"\n",
            "step \"223\" -> training loss \"3.8818\" - sim_NCC \"-0.094689\" - Jdet \"0.0496011972\" -smo \"3.9765\"\n",
            "step \"224\" -> training loss \"3.4677\" - sim_NCC \"-0.261722\" - Jdet \"0.0329695418\" -smo \"3.7295\"\n",
            "step \"225\" -> training loss \"3.4853\" - sim_NCC \"-0.198192\" - Jdet \"0.0442580208\" -smo \"3.6835\"\n",
            "step \"226\" -> training loss \"3.4450\" - sim_NCC \"-0.183356\" - Jdet \"0.0311575551\" -smo \"3.6284\"\n",
            "step \"227\" -> training loss \"3.1427\" - sim_NCC \"-0.242643\" - Jdet \"0.0660181344\" -smo \"3.3854\"\n",
            "step \"228\" -> training loss \"3.2408\" - sim_NCC \"-0.143746\" - Jdet \"0.0542727932\" -smo \"3.3846\"\n",
            "step \"229\" -> training loss \"3.0772\" - sim_NCC \"-0.302294\" - Jdet \"0.0271873642\" -smo \"3.3795\"\n",
            "step \"230\" -> training loss \"2.8604\" - sim_NCC \"-0.226484\" - Jdet \"0.0871082470\" -smo \"3.0869\"\n",
            "step \"231\" -> training loss \"3.0570\" - sim_NCC \"-0.196978\" - Jdet \"0.0470832884\" -smo \"3.2540\"\n",
            "step \"232\" -> training loss \"2.9856\" - sim_NCC \"-0.153599\" - Jdet \"0.0678545684\" -smo \"3.1392\"\n",
            "step \"233\" -> training loss \"3.0203\" - sim_NCC \"-0.115290\" - Jdet \"0.0573735610\" -smo \"3.1355\"\n",
            "step \"234\" -> training loss \"2.9077\" - sim_NCC \"-0.093390\" - Jdet \"0.0860172212\" -smo \"3.0011\"\n",
            "step \"235\" -> training loss \"3.0202\" - sim_NCC \"-0.092488\" - Jdet \"0.0292746555\" -smo \"3.1127\"\n",
            "step \"236\" -> training loss \"2.7918\" - sim_NCC \"-0.231996\" - Jdet \"0.0283628684\" -smo \"3.0238\"\n",
            "step \"237\" -> training loss \"0.6591\" - sim_NCC \"-2.393220\" - Jdet \"0.0250577852\" -smo \"3.0523\"\n",
            "step \"238\" -> training loss \"2.7757\" - sim_NCC \"-0.190671\" - Jdet \"0.0582903922\" -smo \"2.9664\"\n",
            "step \"239\" -> training loss \"2.7255\" - sim_NCC \"-0.230291\" - Jdet \"0.0513392799\" -smo \"2.9558\"\n",
            "step \"240\" -> training loss \"3.0603\" - sim_NCC \"-0.093187\" - Jdet \"0.0438155793\" -smo \"3.1535\"\n",
            "step \"241\" -> training loss \"2.2494\" - sim_NCC \"-1.016597\" - Jdet \"0.0264897458\" -smo \"3.2660\"\n",
            "step \"242\" -> training loss \"3.0432\" - sim_NCC \"-0.221806\" - Jdet \"0.0421487652\" -smo \"3.2650\"\n",
            "step \"243\" -> training loss \"2.9755\" - sim_NCC \"-0.208799\" - Jdet \"0.0662590861\" -smo \"3.1843\"\n",
            "step \"244\" -> training loss \"2.7437\" - sim_NCC \"-0.722160\" - Jdet \"0.0280203633\" -smo \"3.4658\"\n",
            "step \"245\" -> training loss \"3.1160\" - sim_NCC \"-0.239305\" - Jdet \"0.0565532856\" -smo \"3.3553\"\n",
            "step \"246\" -> training loss \"3.1851\" - sim_NCC \"-0.101291\" - Jdet \"0.0778868943\" -smo \"3.2864\"\n",
            "step \"247\" -> training loss \"3.2991\" - sim_NCC \"-0.185348\" - Jdet \"0.0620457567\" -smo \"3.4845\"\n",
            "step \"248\" -> training loss \"3.3447\" - sim_NCC \"-0.210627\" - Jdet \"0.0329567604\" -smo \"3.5553\"\n",
            "step \"249\" -> training loss \"3.3624\" - sim_NCC \"-0.183946\" - Jdet \"0.0339918807\" -smo \"3.5463\"\n",
            "step \"250\" -> training loss \"3.1735\" - sim_NCC \"-0.247960\" - Jdet \"0.0581856631\" -smo \"3.4215\"\n",
            "step \"251\" -> training loss \"3.2443\" - sim_NCC \"-0.231038\" - Jdet \"0.0672121197\" -smo \"3.4753\"\n",
            "step \"252\" -> training loss \"3.2987\" - sim_NCC \"-0.203892\" - Jdet \"0.0429916456\" -smo \"3.5026\"\n",
            "step \"253\" -> training loss \"3.1340\" - sim_NCC \"-0.187632\" - Jdet \"0.0620122664\" -smo \"3.3216\"\n",
            "step \"254\" -> training loss \"3.0020\" - sim_NCC \"-0.217250\" - Jdet \"0.0742467120\" -smo \"3.2193\"\n",
            "step \"255\" -> training loss \"3.2213\" - sim_NCC \"-0.201935\" - Jdet \"0.0464671403\" -smo \"3.4233\"\n",
            "step \"256\" -> training loss \"2.9832\" - sim_NCC \"-0.178694\" - Jdet \"0.0737140253\" -smo \"3.1619\"\n",
            "step \"257\" -> training loss \"2.9893\" - sim_NCC \"-0.209705\" - Jdet \"0.0752830654\" -smo \"3.1990\"\n",
            "step \"258\" -> training loss \"3.1115\" - sim_NCC \"-0.231139\" - Jdet \"0.0304507725\" -smo \"3.3427\"\n",
            "step \"259\" -> training loss \"1.3318\" - sim_NCC \"-2.017904\" - Jdet \"0.0278864950\" -smo \"3.3497\"\n",
            "step \"260\" -> training loss \"3.1695\" - sim_NCC \"-0.221355\" - Jdet \"0.0282910559\" -smo \"3.3908\"\n",
            "step \"261\" -> training loss \"3.1837\" - sim_NCC \"-0.197005\" - Jdet \"0.0422188640\" -smo \"3.3807\"\n",
            "step \"262\" -> training loss \"2.1017\" - sim_NCC \"-1.461180\" - Jdet \"0.0303046796\" -smo \"3.5629\"\n",
            "step \"263\" -> training loss \"3.8829\" - sim_NCC \"-0.199077\" - Jdet \"0.0756913200\" -smo \"4.0819\"\n",
            "step \"264\" -> training loss \"4.7163\" - sim_NCC \"-0.226423\" - Jdet \"0.0761421695\" -smo \"4.9427\"\n",
            "step \"265\" -> training loss \"7.0095\" - sim_NCC \"-0.253330\" - Jdet \"0.0814297199\" -smo \"7.2628\"\n",
            "step \"266\" -> training loss \"8.1369\" - sim_NCC \"-0.210351\" - Jdet \"0.1516323239\" -smo \"8.3472\"\n",
            "step \"267\" -> training loss \"10.0866\" - sim_NCC \"-0.204444\" - Jdet \"0.1212268174\" -smo \"10.2910\"\n",
            "step \"268\" -> training loss \"9.6211\" - sim_NCC \"-0.209323\" - Jdet \"0.1548580378\" -smo \"9.8304\"\n",
            "step \"269\" -> training loss \"9.2412\" - sim_NCC \"-0.175579\" - Jdet \"0.1051213518\" -smo \"9.4167\"\n",
            "step \"270\" -> training loss \"7.2561\" - sim_NCC \"-0.263964\" - Jdet \"0.0801068619\" -smo \"7.5200\"\n",
            "step \"271\" -> training loss \"6.1437\" - sim_NCC \"-0.232604\" - Jdet \"0.1069850698\" -smo \"6.3763\"\n",
            "step \"272\" -> training loss \"5.5645\" - sim_NCC \"-0.589104\" - Jdet \"0.0603842437\" -smo \"6.1536\"\n",
            "step \"273\" -> training loss \"4.3545\" - sim_NCC \"-0.206067\" - Jdet \"0.0599351078\" -smo \"4.5606\"\n",
            "step \"274\" -> training loss \"6.2076\" - sim_NCC \"-0.298992\" - Jdet \"0.0777425617\" -smo \"6.5066\"\n",
            "step \"275\" -> training loss \"13.3290\" - sim_NCC \"-0.226908\" - Jdet \"0.1756473929\" -smo \"13.5560\"\n",
            "step \"276\" -> training loss \"14.5982\" - sim_NCC \"-0.233688\" - Jdet \"0.2189515829\" -smo \"14.8319\"\n",
            "step \"277\" -> training loss \"16.1246\" - sim_NCC \"-2.063509\" - Jdet \"0.2365998775\" -smo \"18.1881\"\n",
            "step \"278\" -> training loss \"20.0076\" - sim_NCC \"-0.205785\" - Jdet \"0.2915636897\" -smo \"20.2134\"\n",
            "step \"279\" -> training loss \"20.5852\" - sim_NCC \"-0.238279\" - Jdet \"0.2962731421\" -smo \"20.8235\"\n",
            "step \"280\" -> training loss \"21.4737\" - sim_NCC \"-0.227408\" - Jdet \"0.2924529016\" -smo \"21.7011\"\n",
            "step \"281\" -> training loss \"22.4944\" - sim_NCC \"-0.168288\" - Jdet \"0.3336539865\" -smo \"22.6627\"\n",
            "step \"282\" -> training loss \"25.3398\" - sim_NCC \"-0.233334\" - Jdet \"0.3647108674\" -smo \"25.5731\"\n",
            "step \"283\" -> training loss \"27.3824\" - sim_NCC \"-1.532046\" - Jdet \"0.3800571859\" -smo \"28.9144\"\n",
            "step \"284\" -> training loss \"30.8623\" - sim_NCC \"-0.194485\" - Jdet \"0.4296997488\" -smo \"31.0568\"\n",
            "step \"285\" -> training loss \"34.7566\" - sim_NCC \"-0.181143\" - Jdet \"0.4743081033\" -smo \"34.9377\"\n",
            "step \"286\" -> training loss \"36.6699\" - sim_NCC \"-0.187402\" - Jdet \"0.5304904580\" -smo \"36.8573\"\n",
            "step \"287\" -> training loss \"40.6645\" - sim_NCC \"-0.181192\" - Jdet \"0.6354669333\" -smo \"40.8456\"\n",
            "step \"288\" -> training loss \"43.8247\" - sim_NCC \"-0.122717\" - Jdet \"0.7465575337\" -smo \"43.9474\"\n",
            "step \"289\" -> training loss \"46.9416\" - sim_NCC \"-0.213777\" - Jdet \"0.7917515635\" -smo \"47.1554\"\n",
            "step \"290\" -> training loss \"47.9814\" - sim_NCC \"-0.210618\" - Jdet \"0.8351801634\" -smo \"48.1920\"\n",
            "step \"291\" -> training loss \"46.3240\" - sim_NCC \"-0.223213\" - Jdet \"0.7761677504\" -smo \"46.5472\"\n",
            "step \"292\" -> training loss \"44.7555\" - sim_NCC \"-0.171423\" - Jdet \"0.7205780149\" -smo \"44.9269\"\n",
            "step \"293\" -> training loss \"41.0371\" - sim_NCC \"-0.246482\" - Jdet \"0.6211789846\" -smo \"41.2836\"\n",
            "step \"294\" -> training loss \"39.8551\" - sim_NCC \"-0.222508\" - Jdet \"0.5787441730\" -smo \"40.0776\"\n",
            "step \"295\" -> training loss \"39.8886\" - sim_NCC \"-0.232639\" - Jdet \"0.5470361114\" -smo \"40.1213\"\n",
            "step \"296\" -> training loss \"39.3429\" - sim_NCC \"-0.246165\" - Jdet \"0.5345181823\" -smo \"39.5891\"\n",
            "step \"297\" -> training loss \"38.5529\" - sim_NCC \"-0.219920\" - Jdet \"0.5278527737\" -smo \"38.7728\"\n",
            "step \"298\" -> training loss \"35.6691\" - sim_NCC \"-0.203754\" - Jdet \"0.5528898239\" -smo \"35.8729\"\n",
            "step \"299\" -> training loss \"35.3094\" - sim_NCC \"-0.093528\" - Jdet \"0.5393984318\" -smo \"35.4029\"\n",
            "step \"300\" -> training loss \"34.0321\" - sim_NCC \"-0.099461\" - Jdet \"0.5306151509\" -smo \"34.1316\"\n",
            "step \"301\" -> training loss \"34.8381\" - sim_NCC \"-0.129414\" - Jdet \"0.5046190619\" -smo \"34.9675\"\n",
            "step \"302\" -> training loss \"33.6464\" - sim_NCC \"-1.145905\" - Jdet \"0.4841345549\" -smo \"34.7923\"\n",
            "step \"303\" -> training loss \"33.2550\" - sim_NCC \"-0.990101\" - Jdet \"0.4761267900\" -smo \"34.2451\"\n",
            "step \"304\" -> training loss \"33.1945\" - sim_NCC \"-0.184862\" - Jdet \"0.4709182382\" -smo \"33.3793\"\n",
            "step \"305\" -> training loss \"32.2126\" - sim_NCC \"-0.687887\" - Jdet \"0.4616312087\" -smo \"32.9005\"\n",
            "step \"306\" -> training loss \"31.8309\" - sim_NCC \"-0.605565\" - Jdet \"0.4551161230\" -smo \"32.4364\"\n",
            "step \"307\" -> training loss \"31.7806\" - sim_NCC \"-0.198372\" - Jdet \"0.4568305314\" -smo \"31.9790\"\n",
            "step \"308\" -> training loss \"30.4167\" - sim_NCC \"-0.228396\" - Jdet \"0.4629084468\" -smo \"30.6451\"\n",
            "step \"309\" -> training loss \"30.7452\" - sim_NCC \"-0.194144\" - Jdet \"0.4599293172\" -smo \"30.9393\"\n",
            "step \"310\" -> training loss \"30.0643\" - sim_NCC \"-0.185299\" - Jdet \"0.4616605639\" -smo \"30.2496\"\n",
            "step \"311\" -> training loss \"29.3451\" - sim_NCC \"-0.094014\" - Jdet \"0.4691555500\" -smo \"29.4391\"\n",
            "step \"312\" -> training loss \"29.5258\" - sim_NCC \"-0.097925\" - Jdet \"0.4431366920\" -smo \"29.6237\"\n",
            "step \"313\" -> training loss \"28.3822\" - sim_NCC \"-0.962039\" - Jdet \"0.4097837210\" -smo \"29.3443\"\n",
            "step \"314\" -> training loss \"28.0453\" - sim_NCC \"-0.093919\" - Jdet \"0.4252444506\" -smo \"28.1392\"\n",
            "step \"315\" -> training loss \"28.5561\" - sim_NCC \"-0.185550\" - Jdet \"0.4097351134\" -smo \"28.7417\"\n",
            "step \"316\" -> training loss \"28.1017\" - sim_NCC \"-0.226536\" - Jdet \"0.3999936581\" -smo \"28.3282\"\n",
            "step \"317\" -> training loss \"27.6038\" - sim_NCC \"-0.106038\" - Jdet \"0.4128559232\" -smo \"27.7098\"\n",
            "step \"318\" -> training loss \"27.1886\" - sim_NCC \"-0.134039\" - Jdet \"0.4114599228\" -smo \"27.3226\"\n",
            "step \"319\" -> training loss \"26.3638\" - sim_NCC \"-0.194152\" - Jdet \"0.4008368552\" -smo \"26.5579\"\n",
            "step \"320\" -> training loss \"26.4640\" - sim_NCC \"-0.222963\" - Jdet \"0.3839131296\" -smo \"26.6870\"\n",
            "step \"321\" -> training loss \"25.7724\" - sim_NCC \"-0.085759\" - Jdet \"0.4021050036\" -smo \"25.8581\"\n",
            "step \"322\" -> training loss \"25.2793\" - sim_NCC \"-0.213858\" - Jdet \"0.3745261133\" -smo \"25.4931\"\n",
            "step \"323\" -> training loss \"24.9238\" - sim_NCC \"-0.168119\" - Jdet \"0.3810956776\" -smo \"25.0919\"\n",
            "step \"324\" -> training loss \"21.5735\" - sim_NCC \"-3.329128\" - Jdet \"0.3598030806\" -smo \"24.9026\"\n",
            "step \"325\" -> training loss \"24.4815\" - sim_NCC \"-0.074112\" - Jdet \"0.3729429841\" -smo \"24.5556\"\n",
            "step \"326\" -> training loss \"24.5176\" - sim_NCC \"-0.131394\" - Jdet \"0.3776475489\" -smo \"24.6490\"\n",
            "step \"327\" -> training loss \"24.3958\" - sim_NCC \"-0.201190\" - Jdet \"0.3615902066\" -smo \"24.5970\"\n",
            "step \"328\" -> training loss \"24.3594\" - sim_NCC \"-0.222687\" - Jdet \"0.3574206829\" -smo \"24.5821\"\n",
            "step \"329\" -> training loss \"23.9264\" - sim_NCC \"-0.080135\" - Jdet \"0.3887736797\" -smo \"24.0066\"\n",
            "step \"330\" -> training loss \"23.6196\" - sim_NCC \"-0.225379\" - Jdet \"0.3756712079\" -smo \"23.8450\"\n",
            "step \"331\" -> training loss \"24.1495\" - sim_NCC \"-0.236673\" - Jdet \"0.3550535142\" -smo \"24.3862\"\n",
            "step \"332\" -> training loss \"23.3555\" - sim_NCC \"-0.222154\" - Jdet \"0.3780183792\" -smo \"23.5777\"\n",
            "step \"333\" -> training loss \"24.0153\" - sim_NCC \"-0.223787\" - Jdet \"0.3595970273\" -smo \"24.2391\"\n",
            "step \"334\" -> training loss \"23.8909\" - sim_NCC \"-0.196942\" - Jdet \"0.3540278971\" -smo \"24.0878\"\n",
            "step \"335\" -> training loss \"23.2366\" - sim_NCC \"-0.236217\" - Jdet \"0.3734827936\" -smo \"23.4728\"\n",
            "step \"336\" -> training loss \"23.2837\" - sim_NCC \"-0.194783\" - Jdet \"0.3699710667\" -smo \"23.4785\"\n",
            "step \"337\" -> training loss \"23.7817\" - sim_NCC \"-0.259616\" - Jdet \"0.3492901027\" -smo \"24.0413\"\n",
            "step \"338\" -> training loss \"23.1590\" - sim_NCC \"-0.204726\" - Jdet \"0.3625939786\" -smo \"23.3638\"\n",
            "step \"339\" -> training loss \"23.2743\" - sim_NCC \"-0.251431\" - Jdet \"0.3541686237\" -smo \"23.5258\"\n",
            "step \"340\" -> training loss \"23.8016\" - sim_NCC \"-0.255497\" - Jdet \"0.3469803929\" -smo \"24.0571\"\n",
            "step \"341\" -> training loss \"23.7580\" - sim_NCC \"-0.202907\" - Jdet \"0.3462547660\" -smo \"23.9609\"\n",
            "step \"342\" -> training loss \"23.6035\" - sim_NCC \"-0.364737\" - Jdet \"0.3428861797\" -smo \"23.9682\"\n",
            "step \"343\" -> training loss \"23.6135\" - sim_NCC \"-0.172565\" - Jdet \"0.3487253189\" -smo \"23.7860\"\n",
            "step \"344\" -> training loss \"23.6284\" - sim_NCC \"-0.084085\" - Jdet \"0.3562657237\" -smo \"23.7124\"\n",
            "step \"345\" -> training loss \"23.5474\" - sim_NCC \"-0.289956\" - Jdet \"0.3429858088\" -smo \"23.8374\"\n",
            "step \"346\" -> training loss \"22.9115\" - sim_NCC \"-0.215385\" - Jdet \"0.3520815969\" -smo \"23.1269\"\n",
            "step \"347\" -> training loss \"23.2188\" - sim_NCC \"-0.129946\" - Jdet \"0.3604243994\" -smo \"23.3488\"\n",
            "step \"348\" -> training loss \"22.1747\" - sim_NCC \"-1.447310\" - Jdet \"0.3424622715\" -smo \"23.6221\"\n",
            "step \"349\" -> training loss \"23.2429\" - sim_NCC \"-0.212203\" - Jdet \"0.3441413343\" -smo \"23.4551\"\n",
            "step \"350\" -> training loss \"23.2194\" - sim_NCC \"-0.220688\" - Jdet \"0.3396105170\" -smo \"23.4401\"\n",
            "step \"351\" -> training loss \"22.7095\" - sim_NCC \"-0.214565\" - Jdet \"0.3470787704\" -smo \"22.9240\"\n",
            "step \"352\" -> training loss \"23.1556\" - sim_NCC \"-0.234212\" - Jdet \"0.3352174163\" -smo \"23.3898\"\n",
            "step \"353\" -> training loss \"22.9915\" - sim_NCC \"-0.208309\" - Jdet \"0.3382042348\" -smo \"23.1998\"\n",
            "step \"354\" -> training loss \"22.1966\" - sim_NCC \"-0.212529\" - Jdet \"0.3529112339\" -smo \"22.4091\"\n",
            "step \"355\" -> training loss \"22.9852\" - sim_NCC \"-0.166672\" - Jdet \"0.3366093934\" -smo \"23.1519\"\n",
            "step \"356\" -> training loss \"22.4476\" - sim_NCC \"-0.174505\" - Jdet \"0.3392344117\" -smo \"22.6221\"\n",
            "step \"357\" -> training loss \"22.0474\" - sim_NCC \"-0.238948\" - Jdet \"0.3460835814\" -smo \"22.2863\"\n",
            "step \"358\" -> training loss \"21.8590\" - sim_NCC \"-0.066360\" - Jdet \"0.3532607257\" -smo \"21.9254\"\n",
            "step \"359\" -> training loss \"21.7279\" - sim_NCC \"-0.195147\" - Jdet \"0.3405761123\" -smo \"21.9230\"\n",
            "step \"360\" -> training loss \"21.6112\" - sim_NCC \"-0.206123\" - Jdet \"0.3321525455\" -smo \"21.8173\"\n",
            "step \"361\" -> training loss \"21.5694\" - sim_NCC \"-0.171201\" - Jdet \"0.3285495937\" -smo \"21.7406\"\n",
            "step \"362\" -> training loss \"21.5845\" - sim_NCC \"-0.211394\" - Jdet \"0.3147865832\" -smo \"21.7959\"\n",
            "step \"363\" -> training loss \"21.6078\" - sim_NCC \"-0.132551\" - Jdet \"0.3142208755\" -smo \"21.7404\"\n",
            "step \"364\" -> training loss \"21.0441\" - sim_NCC \"-0.198238\" - Jdet \"0.3299014866\" -smo \"21.2423\"\n",
            "step \"365\" -> training loss \"20.9828\" - sim_NCC \"-0.229489\" - Jdet \"0.3236059546\" -smo \"21.2123\"\n",
            "step \"366\" -> training loss \"21.3392\" - sim_NCC \"-0.209310\" - Jdet \"0.3071720600\" -smo \"21.5485\"\n",
            "step \"367\" -> training loss \"20.9202\" - sim_NCC \"-0.204698\" - Jdet \"0.3234163225\" -smo \"21.1249\"\n",
            "step \"368\" -> training loss \"20.9065\" - sim_NCC \"-0.157342\" - Jdet \"0.3334008753\" -smo \"21.0638\"\n",
            "step \"369\" -> training loss \"20.9901\" - sim_NCC \"-0.260004\" - Jdet \"0.3087012768\" -smo \"21.2501\"\n",
            "step \"370\" -> training loss \"21.1429\" - sim_NCC \"-0.223801\" - Jdet \"0.3038896024\" -smo \"21.3667\"\n",
            "step \"371\" -> training loss \"20.8126\" - sim_NCC \"-0.215026\" - Jdet \"0.3112833202\" -smo \"21.0276\"\n",
            "step \"372\" -> training loss \"21.0007\" - sim_NCC \"-0.258247\" - Jdet \"0.3001538813\" -smo \"21.2589\"\n",
            "step \"373\" -> training loss \"20.9326\" - sim_NCC \"-0.216344\" - Jdet \"0.3046209812\" -smo \"21.1489\"\n",
            "step \"374\" -> training loss \"20.7990\" - sim_NCC \"-0.135427\" - Jdet \"0.3209290802\" -smo \"20.9344\"\n",
            "step \"375\" -> training loss \"20.5326\" - sim_NCC \"-0.190714\" - Jdet \"0.3240101933\" -smo \"20.7233\"\n",
            "step \"376\" -> training loss \"20.4698\" - sim_NCC \"-0.216134\" - Jdet \"0.3216295838\" -smo \"20.6860\"\n",
            "step \"377\" -> training loss \"20.7395\" - sim_NCC \"-0.419717\" - Jdet \"0.2971034944\" -smo \"21.1592\"\n",
            "step \"378\" -> training loss \"20.5041\" - sim_NCC \"-0.208594\" - Jdet \"0.3186368644\" -smo \"20.7126\"\n",
            "step \"379\" -> training loss \"20.8046\" - sim_NCC \"-0.260480\" - Jdet \"0.2987651527\" -smo \"21.0651\"\n",
            "step \"380\" -> training loss \"20.8413\" - sim_NCC \"-0.177711\" - Jdet \"0.3009803593\" -smo \"21.0190\"\n",
            "step \"381\" -> training loss \"20.3532\" - sim_NCC \"-0.662438\" - Jdet \"0.2990269065\" -smo \"21.0156\"\n",
            "step \"382\" -> training loss \"20.4296\" - sim_NCC \"-0.202672\" - Jdet \"0.3186238110\" -smo \"20.6323\"\n",
            "step \"383\" -> training loss \"20.5256\" - sim_NCC \"-0.129337\" - Jdet \"0.3199971318\" -smo \"20.6549\"\n",
            "step \"384\" -> training loss \"20.6847\" - sim_NCC \"-0.338185\" - Jdet \"0.2947055399\" -smo \"21.0229\"\n",
            "step \"385\" -> training loss \"20.7154\" - sim_NCC \"-0.217743\" - Jdet \"0.3008759320\" -smo \"20.9332\"\n",
            "step \"386\" -> training loss \"20.8050\" - sim_NCC \"-0.116609\" - Jdet \"0.3021094203\" -smo \"20.9216\"\n",
            "step \"387\" -> training loss \"20.1800\" - sim_NCC \"-0.803413\" - Jdet \"0.2963904738\" -smo \"20.9834\"\n",
            "step \"388\" -> training loss \"20.3401\" - sim_NCC \"-0.231789\" - Jdet \"0.3149485290\" -smo \"20.5719\"\n",
            "step \"389\" -> training loss \"20.5451\" - sim_NCC \"-0.222118\" - Jdet \"0.3066167831\" -smo \"20.7672\"\n",
            "step \"390\" -> training loss \"20.3555\" - sim_NCC \"-0.192800\" - Jdet \"0.3196559548\" -smo \"20.5483\"\n",
            "step \"391\" -> training loss \"20.5622\" - sim_NCC \"-0.217545\" - Jdet \"0.3010481298\" -smo \"20.7797\"\n",
            "step \"392\" -> training loss \"19.4049\" - sim_NCC \"-1.574470\" - Jdet \"0.2956111431\" -smo \"20.9793\"\n",
            "step \"393\" -> training loss \"20.5540\" - sim_NCC \"-0.204886\" - Jdet \"0.3051861823\" -smo \"20.7589\"\n",
            "step \"394\" -> training loss \"20.6232\" - sim_NCC \"-0.257872\" - Jdet \"0.2982094884\" -smo \"20.8811\"\n",
            "step \"395\" -> training loss \"20.3613\" - sim_NCC \"-0.165852\" - Jdet \"0.3180928826\" -smo \"20.5271\"\n",
            "step \"396\" -> training loss \"20.3954\" - sim_NCC \"-0.204969\" - Jdet \"0.3158985972\" -smo \"20.6004\"\n",
            "step \"397\" -> training loss \"20.3925\" - sim_NCC \"-0.584800\" - Jdet \"0.2975368500\" -smo \"20.9773\"\n",
            "step \"398\" -> training loss \"20.7255\" - sim_NCC \"-0.209940\" - Jdet \"0.3011929989\" -smo \"20.9354\"\n",
            "step \"399\" -> training loss \"20.7606\" - sim_NCC \"-0.214275\" - Jdet \"0.2973641157\" -smo \"20.9748\"\n",
            "step \"400\" -> training loss \"20.3291\" - sim_NCC \"-0.197801\" - Jdet \"0.3134885728\" -smo \"20.5269\"\n",
            "step \"401\" -> training loss \"19.9164\" - sim_NCC \"-1.070271\" - Jdet \"0.2962341905\" -smo \"20.9867\"\n",
            "step \"402\" -> training loss \"20.6766\" - sim_NCC \"-0.187147\" - Jdet \"0.2997831404\" -smo \"20.8638\"\n",
            "step \"403\" -> training loss \"20.3050\" - sim_NCC \"-0.236615\" - Jdet \"0.3153693676\" -smo \"20.5417\"\n",
            "step \"404\" -> training loss \"20.6593\" - sim_NCC \"-0.093534\" - Jdet \"0.3060222268\" -smo \"20.7528\"\n",
            "step \"405\" -> training loss \"20.2457\" - sim_NCC \"-0.200017\" - Jdet \"0.3210343122\" -smo \"20.4457\"\n",
            "step \"406\" -> training loss \"20.4391\" - sim_NCC \"-0.577343\" - Jdet \"0.2939808369\" -smo \"21.0165\"\n",
            "step \"407\" -> training loss \"20.2684\" - sim_NCC \"-0.179585\" - Jdet \"0.3167525530\" -smo \"20.4480\"\n",
            "step \"408\" -> training loss \"20.2935\" - sim_NCC \"-0.198168\" - Jdet \"0.3155387640\" -smo \"20.4917\"\n",
            "step \"409\" -> training loss \"20.6005\" - sim_NCC \"-0.093344\" - Jdet \"0.3093390763\" -smo \"20.6939\"\n",
            "step \"410\" -> training loss \"20.6903\" - sim_NCC \"-0.232490\" - Jdet \"0.3007940948\" -smo \"20.9228\"\n",
            "step \"411\" -> training loss \"20.4074\" - sim_NCC \"-0.162101\" - Jdet \"0.3148195744\" -smo \"20.5695\"\n",
            "step \"412\" -> training loss \"20.5455\" - sim_NCC \"-0.221488\" - Jdet \"0.3022141159\" -smo \"20.7670\"\n",
            "step \"413\" -> training loss \"20.6551\" - sim_NCC \"-0.300077\" - Jdet \"0.2951321304\" -smo \"20.9551\"\n",
            "step \"414\" -> training loss \"20.6631\" - sim_NCC \"-0.284553\" - Jdet \"0.2954441607\" -smo \"20.9477\"\n",
            "step \"415\" -> training loss \"20.4402\" - sim_NCC \"-0.203303\" - Jdet \"0.3083113730\" -smo \"20.6435\"\n",
            "step \"416\" -> training loss \"20.4134\" - sim_NCC \"-0.227725\" - Jdet \"0.3049147427\" -smo \"20.6411\"\n",
            "step \"417\" -> training loss \"20.6742\" - sim_NCC \"-0.273889\" - Jdet \"0.2957700193\" -smo \"20.9481\"\n",
            "step \"418\" -> training loss \"20.3119\" - sim_NCC \"-0.222036\" - Jdet \"0.3096291721\" -smo \"20.5339\"\n",
            "step \"419\" -> training loss \"20.4268\" - sim_NCC \"-0.231992\" - Jdet \"0.3081816733\" -smo \"20.6588\"\n",
            "step \"420\" -> training loss \"20.4096\" - sim_NCC \"-0.091710\" - Jdet \"0.3142092526\" -smo \"20.5013\"\n",
            "step \"421\" -> training loss \"20.6212\" - sim_NCC \"-0.135589\" - Jdet \"0.3047926128\" -smo \"20.7568\"\n",
            "step \"422\" -> training loss \"20.5749\" - sim_NCC \"-0.373484\" - Jdet \"0.2946637273\" -smo \"20.9484\"\n",
            "step \"423\" -> training loss \"20.8228\" - sim_NCC \"-0.103999\" - Jdet \"0.2973736227\" -smo \"20.9268\"\n",
            "step \"424\" -> training loss \"20.3178\" - sim_NCC \"-0.226398\" - Jdet \"0.3107817173\" -smo \"20.5442\"\n",
            "step \"425\" -> training loss \"20.3678\" - sim_NCC \"-0.247023\" - Jdet \"0.3033367395\" -smo \"20.6149\"\n",
            "step \"426\" -> training loss \"20.6986\" - sim_NCC \"-0.179468\" - Jdet \"0.2971547544\" -smo \"20.8780\"\n",
            "step \"427\" -> training loss \"20.6065\" - sim_NCC \"-0.212212\" - Jdet \"0.3003433645\" -smo \"20.8187\"\n",
            "step \"428\" -> training loss \"20.4121\" - sim_NCC \"-0.208454\" - Jdet \"0.3058693409\" -smo \"20.6206\"\n",
            "step \"429\" -> training loss \"20.3183\" - sim_NCC \"-0.093223\" - Jdet \"0.3164440989\" -smo \"20.4115\"\n",
            "step \"430\" -> training loss \"20.3414\" - sim_NCC \"-0.213747\" - Jdet \"0.3052741587\" -smo \"20.5551\"\n",
            "step \"431\" -> training loss \"20.3466\" - sim_NCC \"-0.221140\" - Jdet \"0.3066769540\" -smo \"20.5678\"\n",
            "step \"432\" -> training loss \"20.5430\" - sim_NCC \"-0.185159\" - Jdet \"0.3001922667\" -smo \"20.7281\"\n",
            "step \"433\" -> training loss \"20.6752\" - sim_NCC \"-0.183174\" - Jdet \"0.2965840101\" -smo \"20.8584\"\n",
            "step \"434\" -> training loss \"20.2140\" - sim_NCC \"-0.220019\" - Jdet \"0.3095945716\" -smo \"20.4340\"\n",
            "step \"435\" -> training loss \"20.7066\" - sim_NCC \"-0.180984\" - Jdet \"0.2955873013\" -smo \"20.8876\"\n",
            "step \"436\" -> training loss \"20.3363\" - sim_NCC \"-0.219251\" - Jdet \"0.3077967167\" -smo \"20.5556\"\n",
            "step \"437\" -> training loss \"20.3942\" - sim_NCC \"-0.210588\" - Jdet \"0.3029485941\" -smo \"20.6048\"\n",
            "step \"438\" -> training loss \"20.5629\" - sim_NCC \"-0.229120\" - Jdet \"0.2976492345\" -smo \"20.7920\"\n",
            "step \"439\" -> training loss \"20.1875\" - sim_NCC \"-0.215537\" - Jdet \"0.3096961975\" -smo \"20.4030\"\n",
            "step \"440\" -> training loss \"20.5637\" - sim_NCC \"-0.208253\" - Jdet \"0.2984297574\" -smo \"20.7720\"\n",
            "step \"441\" -> training loss \"20.4113\" - sim_NCC \"-0.094201\" - Jdet \"0.3067598343\" -smo \"20.5055\"\n",
            "step \"442\" -> training loss \"20.5768\" - sim_NCC \"-0.343626\" - Jdet \"0.2931388915\" -smo \"20.9204\"\n",
            "step \"443\" -> training loss \"20.1283\" - sim_NCC \"-0.196991\" - Jdet \"0.3123258352\" -smo \"20.3253\"\n",
            "step \"444\" -> training loss \"20.6592\" - sim_NCC \"-0.138496\" - Jdet \"0.2960156202\" -smo \"20.7977\"\n",
            "step \"445\" -> training loss \"20.1627\" - sim_NCC \"-0.197628\" - Jdet \"0.3122395575\" -smo \"20.3603\"\n",
            "step \"446\" -> training loss \"20.5489\" - sim_NCC \"-0.205749\" - Jdet \"0.2986049056\" -smo \"20.7546\"\n",
            "step \"447\" -> training loss \"20.4444\" - sim_NCC \"-0.242505\" - Jdet \"0.2986246049\" -smo \"20.6869\"\n",
            "step \"448\" -> training loss \"20.1409\" - sim_NCC \"-0.203581\" - Jdet \"0.3094817996\" -smo \"20.3445\"\n",
            "step \"449\" -> training loss \"20.2681\" - sim_NCC \"-0.090862\" - Jdet \"0.3100054562\" -smo \"20.3590\"\n",
            "step \"450\" -> training loss \"20.5461\" - sim_NCC \"-0.253961\" - Jdet \"0.2938911319\" -smo \"20.8001\"\n",
            "step \"451\" -> training loss \"20.3410\" - sim_NCC \"-0.137576\" - Jdet \"0.3032599390\" -smo \"20.4786\"\n",
            "step \"452\" -> training loss \"20.1494\" - sim_NCC \"-0.210426\" - Jdet \"0.3063150346\" -smo \"20.3598\"\n",
            "step \"453\" -> training loss \"20.5568\" - sim_NCC \"-0.174878\" - Jdet \"0.2949899435\" -smo \"20.7317\"\n",
            "step \"454\" -> training loss \"20.5218\" - sim_NCC \"-0.258469\" - Jdet \"0.2932836413\" -smo \"20.7803\"\n",
            "step \"455\" -> training loss \"20.3163\" - sim_NCC \"-0.162958\" - Jdet \"0.3026538789\" -smo \"20.4792\"\n",
            "step \"456\" -> training loss \"20.3278\" - sim_NCC \"-0.200312\" - Jdet \"0.2992123067\" -smo \"20.5281\"\n",
            "step \"457\" -> training loss \"20.3086\" - sim_NCC \"-0.134612\" - Jdet \"0.3022797406\" -smo \"20.4432\"\n",
            "step \"458\" -> training loss \"19.3873\" - sim_NCC \"-1.372181\" - Jdet \"0.2923090458\" -smo \"20.7594\"\n",
            "step \"459\" -> training loss \"20.4444\" - sim_NCC \"-0.200314\" - Jdet \"0.2936131656\" -smo \"20.6447\"\n",
            "step \"460\" -> training loss \"20.3564\" - sim_NCC \"-0.236493\" - Jdet \"0.2941862941\" -smo \"20.5929\"\n",
            "step \"461\" -> training loss \"19.9508\" - sim_NCC \"-0.219173\" - Jdet \"0.3065231740\" -smo \"20.1700\"\n",
            "step \"462\" -> training loss \"20.0450\" - sim_NCC \"-0.230440\" - Jdet \"0.3026276529\" -smo \"20.2754\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdfX-MlzG5Cf"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdyfGPNuCeU"
      },
      "source": [
        "#run testing on testing dataset and save results \n",
        "doTesting = 0\n",
        "if doTesting:\n",
        "    import time , shutil, os\n",
        "    from google.colab import files\n",
        "    %matplotlib inline\n",
        "\n",
        "    # testing download a folder and a file \n",
        "\n",
        "    # !apt install nano\n",
        "    # !nano myTxt14042021.txt\n",
        "    #files.upload()  # or use the left side panel \n",
        "    !ls\n",
        "    # work_directory\n",
        "    wdPath = 'drive/MyDrive/LapIRN_org/'\n",
        "    # # dataset\n",
        "    dataset_path = '../../datasets/L2R_Task3_AbdominalCT_160x192x144'    # # dataset\n",
        "    # # 30 images,  image size =  192,160,256, each has its segmentation, the segmentation has 13 segmentation classes,\n",
        "    # images are already resized to fit the GPU memory \n",
        "\n",
        "    scriptPath = wd_path + 'LapIRN/demoLapIRN_org.py'\n",
        "    doTrain     = \" 0 \"\n",
        "    isLocal     = \" 0 \" # use gitlab or local workstation\n",
        "    slvl1       = \" 0 \"  # start epoch lvl1    \n",
        "    slvl2       = \" 0 \"  # start epoch lvl2       \n",
        "    slvl3       = \" 0 \"  # start epoch lvl3     \n",
        "    lvl1        = \" 3000 \"   # number of iterations for first resolution  \n",
        "    lvl2        = \" 3000 \"  # number of iterations for second resolution  \n",
        "    lvl3        = \" 30000 \" # number of iterations for third resolution \n",
        "    checkpoint  = \" 500 \"# model will be saved after each checkpoint steps\n",
        "\n",
        "\n",
        "    #!python $scriptPath $doTrain $slvl1 $slvl2 $slvl3 $lvl1 $lvl2 $lvl3 $checkpoint $wd_path $dataset_path\n",
        "    cmd = \"python \" + scriptPath + \" \" + doTrain + isLocal + slvl1 +slvl2 +slvl3 +lvl1 +lvl2 +lvl3 +checkpoint  + \" \" +wd_path  + \" \" +dataset_path\n",
        "    os.system(cmd)\n",
        "\n",
        "     \n",
        "    # # download the pretrained model\n",
        "    print(\"done! ....................\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OjoHH2uG8Gf"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWrc1W3FG-h7"
      },
      "source": [
        "#Draw results "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}